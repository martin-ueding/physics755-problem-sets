\documentclass[11pt, english, fleqn, DIV=15, headinclude, BCOR=1cm]{scrartcl}

\usepackage[bibatend]{../header}
\usepackage{../my-boxes}

\usepackage{booktabs}

\hypersetup{
    pdftitle=
}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{4}

\subject{physics755 -- Quantum Field Theory}
\ihead{physics755 -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\newcommand\thegroup{Group Tuesday -- Ripunjay Acharya}

\publishers{\thegroup}
\ofoot{\thegroup}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
    \and
    Oleg Hamm
}
\ifoot{Martin Ueding, Oleg Hamm}

\ohead{\rightmark}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        problem & achieved points & possible points \\
        \midrule
        \nameref{homework:1} & & \punkte{15} \\
        \midrule
        total & & \arabic{totalpoints}
    \end{tabular}
\end{center}

\section{Lorentz algebra 2}
\label{homework:1}

\subsection{Rotations and boosts}

\paragraph{Rotation}

The generators were derived on the previous problem set. They are:
\[
    J_{\rho\sigma} = x_{[\sigma} \partial_{\rho]}
\]
where we have used the antisymmetrization notation in the non-idempotent form
since that seems to be used in this class. This uses the Mathematician's
convention of real antisymmetric generators. To get the Physicists's notation,
we add $-\iup$ to the generators and $\iup$ into the exponential map.
\[
    J_{\rho\sigma} = \iup x_{[\rho} \partial_{\sigma]}
\]
Then $L^3$ is given by $\iup x_{[1} \partial_{2]}$. We found it easier to start
with the representation on $\vec x'$ and show that the generator of that
passive transformation is the same as the one given here. So we start with the
rotation $\tens R(\Theta)$ around the $x^3$-axis:
\begin{align*}
    \Phi(x') &= \Phi\del{\tens R^{-1}(\Theta) \cdot \vec x}
    \intertext{%
        Then we can expand this around $\Theta = 0$.
    }
    &= \sum_{n=0}^\infty \frac{1}{n!} \odx{n}{\Phi}{\Theta}(\vec x) \Theta^n
    \intertext{%
        Next we write this in operator form,
    }
    &= \sum_{n=0}^\infty \frac{1}{n!} \Theta^n \odx{n}{}{\Theta} \Phi(\vec x),
    \intertext{%
        and then as an exponential:
    }
    &= \exp\del{\Theta \od{}{\Theta}} \Phi(\vec x),
\end{align*}
We see that the (Physicist's) generator is given by
\begin{align*}
    T &= - \iup \od{}\Theta.
    \intertext{%
        Now we need to apply the chain rule for this expression.
    }
    &= - \iup \od{x'^\mu}{\Theta}(0) \pd{}{x'^\mu}
\end{align*}
The $\vec x'$ is given by
\[
    \vec x' = \tens R^{-1}(\Theta) \cdot \vec x.
\]
The explicit form of this inverse passive transformation looks like the active
rotation around the $x^3$-axis:
\[
    \tens R^{-1}(\Theta) =
    \begin{pmatrix}
        1 & & & \\
          & \cos(\Theta) & -\sin(\Theta) & \\
          & \sin(\Theta) & \cos(\Theta) & \\
          & & & 1 \\
    \end{pmatrix}.
\]
The derivative with respect to $\Theta$ at $\Theta = 0$ is then given by
\[
    \pd{\tens R^{-1}}\Theta (0) =
    \begin{pmatrix}
        0 & & & \\
          & 0 & -1 & \\
          & 1 & 0 & \\
          & & & 0 \\
    \end{pmatrix}.
\]
Contracting this derivative of $\tens R$ with $\vec x$ to give the derivative
of $\vec x'$ gives
\[
    \od{x'^\mu}{\Theta} (0) =
    \begin{pmatrix}
        0 \\ -x^2 \\ x^1 \\ 0
    \end{pmatrix}.
\]
As a last step with put that into the generator expression:
\[
    T = - \iup \od{x'^\mu}{\Theta}(0) \pd{}{x'^\mu}
    = - \iup 
    \begin{pmatrix}
        0 \\ -x^2 \\ x^1 \\ 0
    \end{pmatrix}^\mu \partial_\mu
    = - \iup \sbr{ x^1 \partial_2 - x^2 \partial_1}
    = \iup \sbr{ x_1 \partial_2 - x_2 \partial_1}.
\]
This is the same generator, so the transformation should be the same.

\paragraph{Boost}

We have
\[
    K^3 = J^{03} = \iup x_{[0} \partial_{3]} =  \iup \sbr{x_0 \partial_3 - x_3
    \partial_0}
\]
for the generator in the functional representation.

The same derivation as above can be applied to the boost as well. The general
inverse passive boost is given by:
\[
    \tens B(\eta) =
    \begin{pmatrix}
        \cosh(\eta) & & & -\sinh(\eta) \\
                    & 1 & & \\
                    & & 1 & \\
        -\sinh(\eta) & & & \cosh(\eta)
    \end{pmatrix}
    .
\]
From this, the generator of the matrix is given by:
\[
    - \iup
    \begin{pmatrix}
        0 & & & -1 \\
          & 0 & & \\
          & & 0 & \\
        -1 & & & 0
    \end{pmatrix}.
\]
Contracting that with $x^\mu$ we get:
\[
    \begin{pmatrix}
        -x^3 \\ 0 \\ 0 \\ -x^0
    \end{pmatrix}.
\]
And contracting that with $\vec \partial$ we get
\[
    \iup [x^0 \partial_3 + x^3 \partial_0]
    = 
    \iup [x_0 \partial_3 - x_3 \partial_0].
\]
So this checks out.

\subsection{Commutators}

The definition of the $L^i$ is
\[
    L^i = \frac12 \epsilon^{ijk} J_{jk}.
\]
This can be written as $L = \hodge J$ and therefore also as $\hodge L = J$
which in components is
\[
    \epsilon_{ijk} L^k = P_{ij}.
\]

We are not sure how many indices we may use. We think that it is not possible
to show this without indices at all. We will use the commutator $[x^i,
\partial_j] = \deltaup^i_j$ instead of the full commutator of the $J$ that is
given on the problem set.

\paragraph{Commutations among $L^i$}

\begin{align*}
    \sbr{L^i, L^a}
    &= \epsilon^{ijk} \epsilon^{abc} \sbr{x_j \partial_k, x_b \partial_c}
    \intertext{%
        Now we can use the commutator identity to split this.
    }
    &= \epsilon^{ijk} \epsilon^{abc} \sbr{
        x_j \sbr{\partial_k, x_b \partial_c}
        + \sbr{x_j, x_b \partial_c} \partial_k
    }
    \intertext{%
        We need to split this up again, so we flip the signs in order to use
        the same identity.
    }
    &= - \epsilon^{ijk} \epsilon^{abc} \sbr{
        x_j \sbr{x_b \partial_c, \partial_k}
        + \sbr{x_b \partial_c, x_j} \partial_k
    }
    \intertext{%
        We expand again.
    }
    &= - \epsilon^{ijk} \epsilon^{abc} \sbr{
        x_j x_b \sbr{\partial_c, \partial_k}
        + x_j \sbr{x_b, \partial_k} \partial_c
        + x_b \sbr{\partial_c, x_j} \partial_k
        + \sbr{x_b, x_j} \partial_k \partial_c
    }
    \intertext{%
        We use the commutator of $\vec x$ and $\vec \partial$ mentioned above.
    }
    &= - \epsilon^{ijk} \epsilon^{abc} \sbr{
        x_j \deltaup_{bk} \partial_c
        - x_b \deltaup_{cj} \partial_k
    }
    \intertext{%
        We absorb the minus sign in the ordering within the bracket.
    }
    &= \epsilon^{ijk} \epsilon^{abc} \sbr{
        x_b \deltaup_{cj} \partial_k
        - x_j \deltaup_{bk} \partial_c
    }
    \intertext{%
        Then we factor out.
    }
    &= \epsilon^{ijk} \epsilon^{abc} x_b \deltaup_{cj} \partial_k
    - \epsilon^{ijk} \epsilon^{abc} x_j \deltaup_{bk} \partial_c
    \intertext{%
        Next we can apply the Kronecker symbol.
    }
    &= \epsilon^{ijk} \epsilon^{ab}{}_j x_b \partial_k
    - \epsilon^{ijk} \epsilon^a{}_k{}^c x_j \partial_c
    \intertext{%
        We reorder the indices such that the last indices are contracted.
    }
    &= \epsilon^{kij} \epsilon^{ab}{}_j x_b \partial_k
    - \epsilon^{ijk} \epsilon^{ca}{}_k x_j \partial_c
    \intertext{%
        Then we can contract the two Levi-Civita symbols.
    }
    &= \sbr{\deltaup^{ka} \deltaup^{ib} - \deltaup^{kb} \deltaup^{ia}}
    x_b \partial_k
    - \sbr{\deltaup^{ic} \deltaup^{ja} - \deltaup^{ia} \deltaup^{jc}}
    x_j \partial_c
    \intertext{%
        Then we get four terms.
    }
    &= \deltaup^{ka} \deltaup^{ib} x_b \partial_k
    - \deltaup^{kb} \deltaup^{ia} x_b \partial_k
    - \deltaup^{ic} \deltaup^{ja} x_j \partial_c
    + \deltaup^{ia} \deltaup^{jc} x_j \partial_c
    \intertext{%
        And then we can contract those.
    }
    &= x_i \partial_a
    - \deltaup^{ia} x_f \partial_f
    - x_a \partial_i
    + \deltaup^{ia} x_f \partial_f
    \intertext{%
        The second and third term cancel, the remainder then is just
    }
    &= x^{[i} \partial^{a]}.
    \intertext{%
        And we can write this as
    }
    &= - \iup \epsilon^{iak} L^k.
\end{align*}
The structure constant should be just $\epsilon_{ijk}$, not with a minus sign
to match the algebra $\mathfrak{su}(2)$.

\paragraph{Commutations among $K^i$}

% TODO
\emph{Missing}

\paragraph{Commutations among $J_+^i$}

% TODO
\emph{Missing}

\paragraph{Commutations among $J_-^i$}

% TODO
\emph{Missing}

\paragraph{Commutation of $J_+^i$ and $J_-^i$}

\begin{align*}
    \sbr{\vec J_+, \vec J_-}^i
    &= \sbr{\vec L + \iup \vec K, \vec L - \iup \vec K}^i
    \intertext{%
        We use the bilinearity of the commutator.
    }
    &= \sbr{\vec L, \vec L}^i
    + \sbr{\vec L, - \iup \vec K}^i
    + \sbr{\iup \vec K, \vec L}^i
    + \sbr{\iup \vec K, - \iup \vec K}^i
    \intertext{%
        Then we extract the factors to the front.
    }
    &= \sbr{\vec L, \vec L}^i
    - \iup \sbr{\vec L, \vec K}^i
    + \iup \sbr{\vec K, \vec L}^i
    + \sbr{\vec K,\vec K}^i
    \intertext{%
        The second and first term are the same, so we can join them together.
        The first and last commutator contain the exact same element on the
        left and right side, so they are zero.
    }
    &= - 2 \iup \sbr{\vec L, \vec K}^i
    \intertext{%
        Now we can expand the definitions.
    }
    &= 2 \iup \sbr{\frac12 \epsilon^{ijk} P_{jk}, P^{0i}}
    \intertext{%
        We also expand the $P$.
    }
    &= 2 \iup \sbr{\frac12 \epsilon^{ijk} x_{[j} \partial_{k]}, x^{[0}
    \partial^{i]}}
    \intertext{%
        The Levi-Civita symbol acting on the antisymmetric tensor in the first
        argument gives a factor of 2. We expand the second argument
    }
    &= 2 \iup \epsilon^{ijk} \sbr{x_{j} \partial_{k}, x^{0} \partial^{i} - x^i
    \partial^0}
    \intertext{%
        Now use the bilinearity again.
    }
    &= 2 \iup \epsilon^{ijk} \sbr{
        \sbr{x_{j} \partial_{k}, x^{0} \partial^{i}}
        - \sbr{x_{j} \partial_{k}, \partial^0 x^i}
    }
    \intertext{%
        Since there is only one time like component in each term, we can pull
        that out of the commutator.
    }
    &= 2 \iup \epsilon^{ijk} \sbr{
        \sbr{x_{j} \partial_{k}, \partial^{i}} x^{0}
        - \sbr{x_{j} \partial_{k}, x^i} \partial^0
    }
    \intertext{%
        Then we can also pull out the $\partial_k$ in the first term and the
        $x_j$ in the second.
    }
    &= 2 \iup \epsilon^{ijk} \sbr{
        \sbr{x_{j}, \partial^{i}} \partial_{k} x^{0}
        - x_{j} \sbr{\partial_{k}, x^i} \partial^0
    }
    \intertext{%
        The commutators are simple, we now have:
    }
    &= 2 \iup \epsilon^{ijk} \sbr{
        \deltaup^i_j \partial_{k} x^{0} + x_{j} \deltaup^i_k \partial^0
    }.
    \intertext{%
        We can now replace the indices $j$ and $k$ with $i$ in each term. There
        is no summation on the $i$s!
    }
    &= 2 \iup \epsilon^{iik} \deltaup^i_j x^{0}
    + 2 \iup \epsilon^{iji} x_{j} \partial^0
    \intertext{%
        Since the Levi-Civita symbol is antisymmetric and the index $i$ appears
        twice, this is just zero.
    }
    &= 0
\end{align*}
Therefore $\vec J_+$ and $\vec J_-$ commute.

\subsection{Left and right-handed spinors}

\paragraph{Dimension}

Using the given formula, the dimension is
\[
    \sbr{2 \cdot \frac12 + 1} [2 \cdot 0 + 1] = 2 \cdot 1 = 2.
\]
Those matrices act on vectors from $\C^2$, which has 2 (complex) dimensions.

\paragraph{Action on left/right-handed spinor}

For a simple scalar field we had the Lorentz transformation given in
Equation~(4) on the problem set:
\[
    \Phi \mapsto \exp(- \iup \vec \Theta \cdot \vec L - \iup \vec \beta \cdot
    \vec K) \, \Phi
\]
Using the definitions of $\vec J_+$ and $\vec J_-$, we can rewrite $\vec L$ and
$\vec K$ in terms of those:
\[
    \vec L = \vec J_+ + \vec J_-
    \eqnsep
    \vec K = \frac1\iup \sbr{\vec J_+ - \vec J_-}
\]
This lets us rewrite the mapping.
\begin{align*}
    \Phi
    &\mapsto
    \exp(- \iup \vec \Theta \cdot \vec L - \iup \vec \beta \cdot
    \vec K) \, \Phi \\
    &= \exp\del{-\iup \vec\Theta\cdot\sbr{\vec J_+ + \vec J_-} - \vec
    \beta\cdot\sbr{\vec J_+ - \vec J_-}} \, \Phi \\
    \intertext{%
        We have shown earlier that the $\vec J_+$ and $\vec J_-$ commute with
        each other. This allows us to split the exponent into two parts.
    }
    &= \exp\del{-\iup \vec\Theta\cdot\vec J_+ - \vec \beta\cdot\vec J_+}
    \exp\del{-\iup \vec\Theta\cdot\vec J_- + \vec \beta\cdot \vec J_-}
    \, \Phi
\end{align*}
There are two parts here, belonging to different representations of the Lorentz
group. We therefore have for the left and right-handed spinors:
\[
    \psi_\text L \mapsto \exp\del{-\iup \vec\Theta\cdot\vec J_- + \vec
    \beta\cdot \vec J_-} \,\psi_\text L
    \eqnsep
    \psi_\text R \mapsto \exp\del{-\iup \vec\Theta\cdot\vec J_+ - \vec
    \beta\cdot\vec J_+} \,\psi_\text R
\]

\subsection{Connection of left and right-handed transformations}

\paragraph{First identity}

Show that $\tens\sigma^2 \tens\sigma^i \tens\sigma^2 = - \tens\sigma^{i*}$.

% TODO

\paragraph{Second identity}

Show that $\tens\sigma^2 \vec\Lambda_\text L^* \tens\sigma^2 = \vec\Lambda_\text R$. This
$\vec\Lambda_\text L$ is the result from the previous subsection:
\[
    \vec\Lambda_\text L = \exp\del{-\iup \vec\Theta \cdot \vec J_- + \vec\beta
    \cdot \vec J_-}
\]
We now show the identity:
\begin{align*}
    \tens \sigma^2 \vec \Lambda_\text L^* \tens \sigma^2
    &= \tens \sigma^2 \exp\del{-\iup \vec\Theta \cdot \vec J_- + \vec\beta
    \cdot \vec J_-}^* \tens \sigma^2
    \intertext{%
        We insert $\vec J_- = \vec \sigma / 2$.
    }
    &= \tens \sigma^2 \exp\del{\iup \vec\Theta \cdot \frac{\vec\sigma^*}{2} +
    \vec\beta \cdot \frac{\vec\sigma^*}{2}} \tens \sigma^2
    \intertext{%
        We write the exponential as a power series.
    }
    &= \sum_{n=0}^\infty \frac{1}{n!} \tens \sigma^2 \sbr{\iup \vec\Theta \cdot
    \frac{\vec\sigma^*}{2} +
    \vec\beta \cdot \frac{\vec\sigma^*}{2}}^n \tens \sigma^2
    \intertext{%
        We factor out the Pauli matrices.
    }
    &= \sum_{n=0}^\infty \frac{1}{n!} \tens \sigma^2 \sbr{\sbr{\iup \vec\Theta +
    \vec\beta} \cdot \frac{\vec\sigma^*}{2}}^n \tens \sigma^2
    \intertext{%
        Now we make it a bit more complicated and write the power with a
        product sign. In order to stay out of trouble with $n = 0$, we split
        this off.
    }
    &= \tens 1_2 + \sum_{n=1}^\infty \frac{1}{n!} \tens \sigma^2 \sbr{\prod_{k = 1}^n
    \sbr{\iup \vec\Theta + \vec\beta} \cdot \frac{\vec\sigma^*}{2}} \tens \sigma^2
    \intertext{%
        Since all the Pauli matrices are their own inverses, one can just add
        $\tens \sigma^2\tens \sigma^2$ to every factor. This means that we can extend the
        scope of the product. All the intermediate $\tens \sigma^2$ will cancel each
        other pairwise.
    }
    &= \tens 1_2 + \sum_{n=1}^\infty \frac{1}{n!} \prod_{k = 1}^n \tens \sigma^2 
    \sbr{\iup \vec\Theta + \vec\beta} \cdot \frac{\vec\sigma^*}{2} \tens \sigma^2
    \intertext{%
        However, we can now use the first relation on each summand in the
        scalar product. Therefore we get
    }
    &= \tens 1_2 + \sum_{n=1}^\infty \frac{1}{n!} \prod_{k = 1}^n
    \sbr{-\iup \vec\Theta - \vec\beta} \cdot \frac{\vec\sigma}{2}.
    \intertext{%
        One can write the product as a plain power again, add the $n = 0$ case
        back to the sum and write it as an exponential.
    }
    &= \exp\del{\sbr{-i \vec\Theta - \vec\beta} \cdot \frac{\vec \sigma}{2}}
    \intertext{%
        This is
    }
    &= \vec \Lambda_\text R.
\end{align*}

\paragraph{Left-handed spinor}

The transformation of a left-handed spinor is given by
\begin{align*}
    \psi_\text L &\mapsto \vec\Lambda_\text L \psi_\text L.
    \intertext{%
        We complex conjugate the whole mapping.
    }
    \psi_\text L^* &\mapsto \vec\Lambda_\text L^* \psi_\text L^*
    \intertext{%
        Now we premultiply with $\iup \tens\sigma^2$.
    }
    \iup\tens\sigma^2 \psi_\text L^* &\mapsto \iup\tens\sigma^2 \vec\Lambda_\text L^* \psi_\text L^*
    \intertext{%
        Since $\tens\sigma^2 \tens\sigma^2 = \tens 1_2$, we can add those
        between the transformation and the spinor.
    }
    \iup\tens\sigma^2 \psi_\text L^* &\mapsto \iup\tens\sigma^2 \vec\Lambda_\text L^*
    \tens\sigma^2 \tens\sigma^2 \psi_\text L^*
    \intertext{%
        Then we use the second identity that we have shown in this subsection.
        The $\iup$ commutes with everything, so we move it to the right
        position.
    }
    \iup\tens\sigma^2 \psi_\text L^* &\mapsto \vec\Lambda_\text R
    \tens\sigma^2 \iup \psi_\text L^*
    \intertext{%
        Now one can see the definition of that $\psi_\text L^c$.
    }
    \psi_\text L^c &\mapsto \vec\Lambda_\text R \psi_\text L^c
\end{align*}

\begin{small}
    We do not know what that “$c$” stands for. If it is derived from some word,
    it should have been upright in the previous and following parts.
\end{small}

\paragraph{Right-handed spinor}

This one is very similar to the left-handed one.
\begin{align*}
    \psi_\text R &\mapsto \Lambda_\text R \psi_\text R
    \intertext{%
        We use the second identity in reverse.
    }
    \psi_\text R &\mapsto \sigma^2 \Lambda_\text L^* \sigma^2 \psi_\text R
    \intertext{%
        Then we premultiply with $- \iup \sigma^2$.
    }
    - \iup \sigma^2 \psi_\text R &\mapsto - \iup \Lambda_\text L^* \sigma^2
    \psi_\text R
    \intertext{%
        We complex conjugate the mapping.
    }
    \iup \sigma^{2*} \psi_\text R^* &\mapsto \iup \Lambda_\text L \sigma^{2*}
    \psi_\text R^*
    \intertext{%
        The complex conjugate of $\sigma^2$ is just the negative since it is
        purely imaginary.
    }
    - \iup \sigma^{2} \psi_\text R^* &\mapsto - \iup \Lambda_\text L \sigma^{2}
    \psi_\text R^*
    \intertext{%
        And now we can identify $\psi_\text R^c$ like we did before.
    }
    \psi_\text R^c &\mapsto \Lambda_\text L \psi_\text R^c
\end{align*}

\subsection{Hermitian matrices and four-vectors}

\paragraph{Determinant}

The determinant is shown quickly:
\begin{align*}
    \det(\tens x) &= \sbr{x^0 + x^3} \sbr{x^0 - x^3} - \sbr{x^1 + \iup x^2}
    \sbr{x^1 - \iup x^2} \\
    &= [x^0]^2 - [x^3]^2 - [x^1]^2 - [x^2]^2 \\
    &= [x^0]^2 - [x^1]^2 - [x^2]^2 - [x^3]^2 \\
    &= \diag(1, -1, -1, -1)_{\mu\nu} x^\mu x^\nu
    \intertext{%
        And assuming that we use the metric with signature $-2$, this is
    }
    &= \eta_{\mu\nu} x^\mu x^\nu.
\end{align*}

\paragraph{Symmetric part}

We have to show that the symmetric part of $\sigma \otimes \bar\sigma$ is given
by the twice metric tensor:
\begin{align*}
    \sigma^\mu \bar\sigma^\nu + \sigma^\nu \bar\sigma^\mu &= 2 \eta^{\mu\nu}
    \tens 1_2
    \intertext{%
        We lower the index $\nu$.
    }
    \sigma^\mu \bar\sigma_\nu + \sigma_\nu \bar\sigma^\mu &= 2 \deltaup^\mu_\nu
    \tens 1_2
    \intertext{%
        The following hurts a bit, but using the metric with signature $-2$ we
        can raise the index $\nu$ on again and converting the $\bar\sigma$ into
        $\sigma$ in that process. We just shift the minus sign around. The
        index positions on the left and right do not match any more.
    }
    \sigma^\mu \sigma^\nu + \sigma^\nu \sigma^\mu &= 2 \deltaup^\mu_\nu
    \tens 1_2
\end{align*}
This now is the anticommutation relation of the Pauli matrices. Since $\sigma^i
\sigma^i = \tens 1_2$, the $\mu = \nu$ parts have to hold. If $\mu$ and $\nu$
are different, the product is antisymmetric, so nothing is left. This is
evident in the product of the Pauli matrices:
\[
    \sigma_i \sigma_j = \iup \epsilon_{ijk} \sigma_k + \deltaup_{ij} \tens 1_2.
\]

\paragraph{Trace}

\begin{align*}
    \tr\del{\sigma^\mu \bar\sigma_\nu} &= 2 \delta^\mu_\nu \\
    \intertext{%
        First we raise the index of the $\bar\sigma$ and remove the bar.
    }
    \tr\del{\sigma^\mu \sigma^\nu} &= 2 \delta^\mu_\nu \\
    \intertext{%
        To compute the product, we have to look at temporal and spatial parts
        separately. We write this in the following way: Summands with Latin
        indices apply when $\mu$ and $\nu$ are not zero. If that is the case,
        then $\mu \sim m$ and $\nu \sim n$.
    }
    \tr\del{\iup \epsilon_{mnp} \sigma^m + \deltaup^{mn} \tens 1_2 +
        \deltaup^{\mu0}\deltaup^{0\nu} \tens 1_2} &= 2 \delta^\mu_\nu \\
    \intertext{%
        The single Pauli matrix has no trace. The unit matrix has trace 2. So
        after using the linearity of the trace we are left with the following.
    }
    2 \sbr{\deltaup^{mn} + \deltaup^{\mu0}\deltaup^{0\nu}} &= 2 \delta^\mu_\nu
\end{align*}
All possible values of $\mu$ and $\nu$ are accounted for and this holds.

\paragraph{Basis and components}

Now we have to show that $\tens x = \bar\sigma_\mu x^\mu$. This can be seen by
writing it out:
\[
    \tens x = x^0
    \begin{pmatrix}
        1 & 0 \\ 0 & 1
    \end{pmatrix}
    + x^1
    \begin{pmatrix}
        0 & 1 \\ 1 & 0
    \end{pmatrix}
    + x^2
    \begin{pmatrix}
        0 & -\iup \\ -\iup & 0
    \end{pmatrix}
    + x^3
    \begin{pmatrix}
        1 & 0 \\ 0 & -1
    \end{pmatrix}
\]
The sum indeed is the form of $\tens x$ given on the sheet.

\paragraph{Component projection}

Last we have to show $x^\mu = \frac12 \tr(\sigma^\mu \tens x)$ to get the one
point for this problem.

For $\mu = 0$, this clearly is the case since $\sigma^\mu = \tens 1_2$. For
other $\mu$ we can write
\begin{align*}
    x^i &= \frac12 \tr\del{\sigma^i \tens x} \\
    \intertext{%
        Then we expand $\tens x$ as shown directly before this problem.
    }
    &= \frac12 \tr\del{\sigma^i \bar\sigma_\nu x^\nu} \\
    \intertext{%
        We split temporal and spatial components again, just a little less
        crude here.
    }
    &= \frac12 \tr\del{\sigma^i \bar\sigma_0 x^0 + \sigma^i \bar\sigma_n x^n} \\
    \intertext{%
        The first summand is easy since $\bar\sigma_0$ is the identity. We can
        raise the index and remove the bar.
    }
    &= \frac12 \tr\del{\sigma^i x^0 + \sigma^i \sigma^n x^n} \\
    \intertext{%
        The single Pauli matrix is traceless. We use the product expression for
        the second.
    }
    &= \frac12 \tr\del{\sbr{\epsilon^i{}_{np} \sigma^p + \deltaup^{in} \tens
    1_2} x^n} \\
    \intertext{%
        Again, the single Pauli matrix has no trace, so all is left is the
        identity with the Kronecker symbol. Since that has trace 2, we are left
        with
    }
    &= x^n.
\end{align*}

\subsection{Linear map}

\paragraph{First identity}

We start with the relation that we have to show.
\begin{align*}
    A \bar\sigma_\mu A^\dagger &= \bar\sigma_\nu \Lambda^\nu{}_\mu
    \intertext{%
        Then we premultiply with $\sigma^\lambda$.
    }
    \sigma^\lambda A \bar\sigma_\mu A^\dagger &= \sigma^\lambda \bar\sigma_\nu
    \Lambda^\nu{}_\mu
    \intertext{%
        Now we take the trace of both sides. This turns out matrix equation
        into a scalar equation and we loose information. This is probably a bad
        thing, although we get the right result in the end. This means that
        there is only probable cause, not a rigid proof.
    }
    \tr\del{\sigma^\lambda A \bar\sigma_\mu A^\dagger} &=
    \tr\del{\sigma^\lambda \bar\sigma_\nu \Lambda^\nu{}_\mu}
    \intertext{%
        The left hand side is the definition of $\Lambda$.
    }
    \Lambda^\lambda{}_\mu &=
    \tr\del{\sigma^\lambda \bar\sigma_\nu \Lambda^\nu{}_\mu}
    \intertext{%
        We split up the temporal and spatial parts again. We explicitly write
        out the minus sign in the spatial parts of $\bar\sigma$ instead of
        fiddling with the raising of the indices. Taking the trace is enough of
        sketchy tricks for one derivation. We will still use $\lambda \sim l$
        and $\nu \sim n$ for the spatial parts of those indices, though. The
        four cases that we have are:
        \[
            \sigma^0 \sigma_0 = \tens 1_2
            \eqnsep
            - \sigma^0 \sigma_n = - \sigma_n
            \eqnsep
            \sigma^l \sigma_0 = \sigma^l
            \eqnsep
            - \sigma^l \sigma_n = - \iup \epsilon_{lnk} \sigma^k + \deltaup^l_n \tens
            1_2.
        \]
        Taking the trace only leaves the first or the last term. We therefore have:
    }
    \Lambda^\lambda{}_\mu &=
    \tr\del{\sbr{\deltaup^{\lambda0} \deltaup^{\nu0} + \deltaup^{ln}} \tens 1_2
    \Lambda^\nu{}_\mu}
    \intertext{%
        This then is the desired result,
    }
    \Lambda^\lambda{}_\mu &=
    \deltaup^\lambda_\nu \Lambda^\nu{}_\mu,
    \intertext{%
        which is a true statement:
    }
    \Lambda^\lambda{}_\mu &=
    \Lambda^\lambda{}_\mu.
\end{align*}

\paragraph{Second identity}

%We can do this one quicker by assuming that the first identity holds.
%\begin{align*}
%    A \bar\sigma_\mu A^\dagger &= \bar\sigma_\nu \Lambda^\nu{}_\mu
%\end{align*}

% TODO
\emph{Missing}

\subsection{Transformation of bilinear}

\emph{Missing}

\subsection{Reality condition}

\emph{Missing}

\end{document}

% vim: spell spelllang=en tw=79
